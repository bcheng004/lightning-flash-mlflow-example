{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "interpreter": {
   "hash": "64d303618c8ee9f1cee103d362ed2586b5f50e4b11555251f761d52c2b006ae5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "InteractiveShell.ast_node_interactivity = 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "mlflow_uri = config['mlflow']['mlflow_uri']\n",
    "mlflow_artifact_uri = config['mlflow']['target_mlflow_artifact_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "\n",
    "download_data(config['text-example']['download_url'], 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-e41b4c988c31b728 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-e41b4c988c31b728\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
      "  3%|▎         | 781/22500 [00:00<00:02, 7733.22ex/s]Dataset csv downloaded and prepared to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-e41b4c988c31b728\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
      "100%|██████████| 22500/22500 [00:02<00:00, 7846.37ex/s]\n",
      "100%|██████████| 23/23 [00:29<00:00,  1.30s/ba]\n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-6e2d172732a47933 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-6e2d172732a47933\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
      " 13%|█▎        | 321/2500 [00:00<00:00, 3207.28ex/s]Dataset csv downloaded and prepared to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-6e2d172732a47933\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 4558.29ex/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.26s/ba]\n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-4c287d5e8061dd3c (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-4c287d5e8061dd3c\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
      "  0%|          | 0/2500 [00:00<?, ?ex/s]Dataset csv downloaded and prepared to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-4c287d5e8061dd3c\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5600.90ex/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.09s/ba]\n"
     ]
    }
   ],
   "source": [
    "from flash.text import TextClassificationData, TextClassifier\n",
    "\n",
    "input_fields,target_fields = 'review', 'sentiment'\n",
    "batch_size=512\n",
    "datamodule = TextClassificationData.from_csv(\n",
    "    train_file=config['text-example']['train_file'],\n",
    "    val_file=config['text-example']['val_file'],\n",
    "    test_file=config['text-example']['test_file'],\n",
    "    input_fields=input_fields,\n",
    "    target_fields=target_fields,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(num_classes=datamodule.num_classes, backbone='prajjwal1/bert-mini')"
   ]
  },
  {
   "source": [
    "**Backbone** Can be seen on huggingface: [huggingface-link](https://huggingface.co/prajjwal1/bert-mini)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "username = os.getenv(\"uid\")\n",
    "mlf_logger = MLFlowLogger(experiment_name=f'lightning-flash-{username}-bert', tracking_uri=mlflow_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    " trainer = flash.Trainer(logger=mlf_logger, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Experiment with name lightning-flash-chengb-bert not found. Creating it.\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | metrics | ModuleDict                    | 0     \n",
      "1 | model   | BertForSequenceClassification | 11.2 M\n",
      "----------------------------------------------------------\n",
      "514       Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.684    Total estimated model params size (MB)\n",
      "Epoch 0:  92%|█████████▏| 44/48 [12:48<01:09, 17.46s/it, loss=0.648, v_num=da32, val_accuracy=0.504, val_cross_entropy=0.723, train_accuracy_step=0.666, train_cross_entropy_step=0.628]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 45/48 [13:03<00:52, 17.42s/it, loss=0.648, v_num=da32, val_accuracy=0.504, val_cross_entropy=0.723, train_accuracy_step=0.666, train_cross_entropy_step=0.628]\n",
      "Epoch 0:  96%|█████████▌| 46/48 [13:18<00:34, 17.35s/it, loss=0.648, v_num=da32, val_accuracy=0.504, val_cross_entropy=0.723, train_accuracy_step=0.666, train_cross_entropy_step=0.628]\n",
      "Epoch 0:  98%|█████████▊| 47/48 [13:35<00:17, 17.35s/it, loss=0.648, v_num=da32, val_accuracy=0.504, val_cross_entropy=0.723, train_accuracy_step=0.666, train_cross_entropy_step=0.628]\n",
      "Epoch 0: 100%|██████████| 48/48 [13:53<00:00, 17.36s/it, loss=0.648, v_num=da32, val_accuracy=0.504, val_cross_entropy=0.723, train_accuracy_step=0.666, train_cross_entropy_step=0.628]\n",
      "Epoch 0: 100%|██████████| 48/48 [14:09<00:00, 17.70s/it, loss=0.648, v_num=da32, val_accuracy=0.675, val_cross_entropy=0.612, train_accuracy_step=0.643, train_cross_entropy_step=0.645, train_accuracy_epoch=0.578, train_cross_entropy_epoch=0.718]\n",
      "Epoch 0: 100%|██████████| 48/48 [14:10<00:00, 17.71s/it, loss=0.648, v_num=da32, val_accuracy=0.675, val_cross_entropy=0.612, train_accuracy_step=0.643, train_cross_entropy_step=0.645, train_accuracy_epoch=0.578, train_cross_entropy_epoch=0.718]\n"
     ]
    }
   ],
   "source": [
    "trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 5/5 [01:08<00:00, 13.78s/it]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.6736000180244446, 'test_cross_entropy': 0.6115491986274719}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_accuracy': 0.6736000180244446,\n",
       "  'test_cross_entropy': 0.6115491986274719}]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one is not training on linux blade and training on windows laptop\n",
    "import shutil\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    file_names = os.listdir()\n",
    "    exp_num = '138'\n",
    "    current_artifact_base_fpath = os.getcwd()\n",
    "    source_dir = f'{current_artifact_base_fpath}'\n",
    "    target_dir = mlflow_artifact_uri\n",
    "    for file_name in file_names:\n",
    "        if file_name  == exp_num:\n",
    "            for subfolder in os.listdir(file_name):\n",
    "                for artifacts in os.listdir(os.path.join(file_name,subfolder)):\n",
    "                    if artifacts == 'checkpoints':\n",
    "                        orig_artifacts_loc = os.path.join(os.path.join(os.path.join(source_dir, file_name), subfolder), artifacts)\n",
    "                        # print(orig_artifacts_loc)\n",
    "                        new_artifacts_loc = os.path.join(os.path.join(os.path.join(source_dir, file_name), subfolder), 'artifacts')\n",
    "                        # print(new_artifacts_loc)\n",
    "                        shutil.move(orig_artifacts_loc, new_artifacts_loc)\n",
    "            shutil.move(os.path.join(source_dir, file_name), target_dir)"
   ]
  },
  {
   "source": [
    "#### Take a look at mlflow for best run "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '1b5c9eb454c44109b8977a37d21eda32'\n",
    "model_path = f'{mlflow_artifact_uri}\\\\{exp_num}\\\\{run_id}\\\\artifacts'\n",
    "for pt in os.listdir(model_path):\n",
    "    if pt.endswith('.ckpt'):\n",
    "        ckpt_path = f'{model_path}\\\\{pt}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# pred_datamodule = TextClassificationData.from_csv(\n",
    "#     predict_file=config['text-example']['predict_file'],\n",
    "#     input_fields=input_fields,\n",
    "# )\n",
    "# predictions = flash.Trainer().predict(model, datamodule=pred_datamodule)\n",
    "# print(predictions)\n",
    "\n",
    "predictions = model.predict([\n",
    "    \"Turgid dialogue, feeble characterization - Harvey Keitel a judge?.\",\n",
    "    \"I come from Bulgaria where it 's almost impossible to have a tornado.\"\n",
    "    \"Very, very afraid\"\n",
    "    \"This guy has done a great job with this movie!\",\n",
    "])\n",
    "print(predictions)"
   ]
  }
 ]
}