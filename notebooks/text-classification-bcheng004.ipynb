{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "interpreter": {
   "hash": "64d303618c8ee9f1cee103d362ed2586b5f50e4b11555251f761d52c2b006ae5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "InteractiveShell.ast_node_interactivity = 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "mlflow_artifact_uri = config['mlflow']['target_mlflow_artifact_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "\n",
    "download_data(config['text-example']['download_url'], 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-d32d810218788d66 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-d32d810218788d66\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
      "  7%|▋         | 1481/22500 [00:00<00:01, 14808.67ex/s]Dataset csv downloaded and prepared to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-d32d810218788d66\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
      "100%|██████████| 22500/22500 [00:02<00:00, 11072.92ex/s]\n",
      "100%|██████████| 23/23 [00:25<00:00,  1.11s/ba]\n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-d897953bbb712c45 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-d897953bbb712c45\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
      " 57%|█████▋    | 1437/2500 [00:00<00:00, 14368.10ex/s]Dataset csv downloaded and prepared to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-d897953bbb712c45\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 11961.55ex/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.14ba/s]\n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-dd76e437a1d33404 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-dd76e437a1d33404\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
      " 21%|██▏       | 537/2500 [00:00<00:00, 5264.71ex/s]Dataset csv downloaded and prepared to C:\\Users\\chengb\\.cache\\huggingface\\datasets\\csv\\default-dd76e437a1d33404\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 7021.89ex/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.14ba/s]\n"
     ]
    }
   ],
   "source": [
    "from flash.text import TextClassificationData, TextClassifier\n",
    "\n",
    "input_fields,target_fields = 'review', 'sentiment'\n",
    "batch_size=512\n",
    "datamodule = TextClassificationData.from_csv(\n",
    "    train_file=config['text-example']['train_file'],\n",
    "    val_file=config['text-example']['val_file'],\n",
    "    test_file=config['text-example']['test_file'],\n",
    "    input_fields=input_fields,\n",
    "    target_fields=target_fields,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(num_classes=datamodule.num_classes, backbone='prajjwal1/bert-mini')"
   ]
  },
  {
   "source": [
    "**Backbone** Can be seen on huggingface: [huggingface-link](https://huggingface.co/prajjwal1/bert-mini)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "username = os.getenv(\"uid\")\n",
    "mlf_logger = MLFlowLogger(experiment_name=f'lightning-flash-{username}-tester', tracking_uri=config['mlflow']['mlflow_uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    " trainer = flash.Trainer(logger=mlf_logger, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Experiment with name lightning-flash-chengb-tester not found. Creating it.\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | metrics | ModuleDict                    | 0     \n",
      "1 | model   | BertForSequenceClassification | 11.2 M\n",
      "----------------------------------------------------------\n",
      "514       Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.684    Total estimated model params size (MB)\n",
      "Epoch 0:  92%|█████████▏| 44/48 [13:07<01:11, 17.89s/it, loss=0.638, v_num=c8b3, val_accuracy=0.504, val_cross_entropy=0.710, train_accuracy_step=0.658, train_cross_entropy_step=0.614]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 45/48 [13:28<00:53, 17.96s/it, loss=0.638, v_num=c8b3, val_accuracy=0.504, val_cross_entropy=0.710, train_accuracy_step=0.658, train_cross_entropy_step=0.614]\n",
      "Epoch 0:  96%|█████████▌| 46/48 [13:41<00:35, 17.86s/it, loss=0.638, v_num=c8b3, val_accuracy=0.504, val_cross_entropy=0.710, train_accuracy_step=0.658, train_cross_entropy_step=0.614]\n",
      "Epoch 0:  98%|█████████▊| 47/48 [13:55<00:17, 17.77s/it, loss=0.638, v_num=c8b3, val_accuracy=0.504, val_cross_entropy=0.710, train_accuracy_step=0.658, train_cross_entropy_step=0.614]\n",
      "Epoch 0: 100%|██████████| 48/48 [14:08<00:00, 17.67s/it, loss=0.638, v_num=c8b3, val_accuracy=0.504, val_cross_entropy=0.710, train_accuracy_step=0.658, train_cross_entropy_step=0.614]\n",
      "Epoch 0: 100%|██████████| 48/48 [14:22<00:00, 17.97s/it, loss=0.638, v_num=c8b3, val_accuracy=0.674, val_cross_entropy=0.607, train_accuracy_step=0.662, train_cross_entropy_step=0.617, train_accuracy_epoch=0.585, train_cross_entropy_epoch=0.704]\n",
      "Epoch 0: 100%|██████████| 48/48 [14:23<00:00, 17.98s/it, loss=0.638, v_num=c8b3, val_accuracy=0.674, val_cross_entropy=0.607, train_accuracy_step=0.662, train_cross_entropy_step=0.617, train_accuracy_epoch=0.585, train_cross_entropy_epoch=0.704]\n"
     ]
    }
   ],
   "source": [
    "trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 5/5 [00:51<00:00, 10.25s/it]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.670799970626831, 'test_cross_entropy': 0.6051112413406372}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_accuracy': 0.670799970626831,\n",
       "  'test_cross_entropy': 0.6051112413406372}]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one is not training on linux blade and training on windows laptop\n",
    "import shutil\n",
    "\n",
    "file_names = os.listdir()\n",
    "exp_num = '132'\n",
    "current_artifact_base_fpath = os.getcwd()\n",
    "source_dir = f'{current_artifact_base_fpath}'\n",
    "target_dir = mlflow_artifact_uri\n",
    "for file_name in file_names:\n",
    "    if file_name  == exp_num:\n",
    "        for subfolder in os.listdir(file_name):\n",
    "            for artifacts in os.listdir(os.path.join(file_name,subfolder)):\n",
    "                orig_artifacts_loc = os.path.join(os.path.join(os.path.join(source_dir, file_name), subfolder), artifacts)\n",
    "                # print(orig_artifacts_loc)\n",
    "                new_artifacts_loc = os.path.join(os.path.join(os.path.join(source_dir, file_name), subfolder), 'artifacts')\n",
    "                # print(new_artifacts_loc)\n",
    "                shutil.move(orig_artifacts_loc, new_artifacts_loc)\n",
    "        shutil.move(os.path.join(source_dir, file_name), target_dir)"
   ]
  },
  {
   "source": [
    "#### Take a look at mlflow for best run "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '518e552a871c46549611d1e6d560c8b3'\n",
    "model_path = f'{mlflow_artifact_uri}\\\\{exp_num}\\\\{run_id}\\\\artifacts'\n",
    "for pt in os.listdir(model_path):\n",
    "    if pt.endswith('.ckpt'):\n",
    "        ckpt_path = f'{model_path}\\\\{pt}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# pred_datamodule = TextClassificationData.from_csv(\n",
    "#     predict_file=config['text-example']['predict_file'],\n",
    "#     input_fields=input_fields,\n",
    "# )\n",
    "# predictions = flash.Trainer().predict(model, datamodule=pred_datamodule)\n",
    "# print(predictions)\n",
    "\n",
    "predictions = model.predict([\n",
    "    \"Turgid dialogue, feeble characterization - Harvey Keitel a judge?.\",\n",
    "    \"I come from Bulgaria where it 's almost impossible to have a tornado.\"\n",
    "    \"Very, very afraid\"\n",
    "    \"This guy has done a great job with this movie!\",\n",
    "])\n",
    "print(predictions)"
   ]
  }
 ]
}